# Dataset Configuration for Carbon-Kube Evaluation Framework

datasets:
  # Raw datasets
  raw:
    sample_metrics:
      path: "raw/sample_metrics.csv"
      description: "Sample carbon efficiency metrics from real experiments"
      format: "csv"
      size: "small"
      features:
        - experiment_id
        - timestamp
        - scheduler
        - node_type
        - workload_type
        - carbon_efficiency
        - energy_consumption
        - performance_score
        - cpu_utilization
        - memory_utilization
        - gpu_utilization
        - response_time
        - throughput
        - queue_time
        - completion_rate
        - resource_utilization
      target_variables:
        - carbon_efficiency
        - energy_consumption
        - performance_score
      categorical_features:
        - scheduler
        - node_type
        - workload_type
      numerical_features:
        - cpu_utilization
        - memory_utilization
        - gpu_utilization
        - response_time
        - throughput
        - queue_time
        - completion_rate
        - resource_utilization
      
  # Synthetic datasets (generated)
  synthetic:
    carbon_efficiency_main:
      path: "synthetic/carbon_efficiency_main.csv"
      description: "Main synthetic dataset with comprehensive carbon efficiency metrics"
      format: "csv"
      size: "large"
      samples: 2000
      use_case: "General evaluation and scheduler comparison"
      
    baseline_kubernetes_default:
      path: "synthetic/baseline_kubernetes_default.csv"
      description: "Baseline performance with default Kubernetes scheduler"
      format: "csv"
      size: "medium"
      samples: 500
      use_case: "Baseline comparison"
      
    baseline_carbon_aware_v1:
      path: "synthetic/baseline_carbon_aware_v1.csv"
      description: "Baseline performance with carbon-aware scheduler v1"
      format: "csv"
      size: "medium"
      samples: 500
      use_case: "Baseline comparison"
      
    ablation_without_carbon_awareness:
      path: "synthetic/ablation_without_carbon_awareness.csv"
      description: "Ablation study without carbon awareness feature"
      format: "csv"
      size: "medium"
      samples: 300
      use_case: "Ablation study"
      
    ablation_without_energy_optimization:
      path: "synthetic/ablation_without_energy_optimization.csv"
      description: "Ablation study without energy optimization feature"
      format: "csv"
      size: "medium"
      samples: 300
      use_case: "Ablation study"
      
    timeseries_14days:
      path: "synthetic/timeseries_14days.csv"
      description: "Time series data with daily and weekly patterns"
      format: "csv"
      size: "large"
      samples: 4032
      use_case: "Temporal analysis"
      temporal: true
      
  # Benchmark datasets
  benchmarks:
    benchmark_small:
      path: "benchmarks/benchmark_small.csv"
      description: "Small benchmark dataset for quick testing"
      format: "csv"
      size: "small"
      samples: 100
      use_case: "Unit testing and quick validation"
      
    benchmark_medium:
      path: "benchmarks/benchmark_medium.csv"
      description: "Medium benchmark dataset for development"
      format: "csv"
      size: "medium"
      samples: 1000
      use_case: "Development and integration testing"
      
    benchmark_large:
      path: "benchmarks/benchmark_large.csv"
      description: "Large benchmark dataset for performance testing"
      format: "csv"
      size: "large"
      samples: 10000
      use_case: "Performance and scalability testing"

# Data quality specifications
data_quality:
  missing_values:
    tolerance: 0.05  # 5% missing values allowed
    handling: "imputation"  # or "removal"
    
  outliers:
    detection_method: "iqr"  # or "zscore", "isolation_forest"
    threshold: 3.0
    handling: "cap"  # or "remove", "keep"
    
  duplicates:
    check: true
    handling: "remove"
    
  consistency:
    check_ranges: true
    check_correlations: true
    
# Preprocessing configurations
preprocessing:
  scaling:
    method: "standard"  # or "minmax", "robust"
    features: "numerical"
    
  encoding:
    categorical_method: "onehot"  # or "label", "target"
    handle_unknown: "ignore"
    
  feature_engineering:
    create_ratios: true
    create_interactions: false
    polynomial_features: false
    
# Validation configurations
validation:
  train_test_split: 0.8
  cross_validation:
    method: "kfold"
    folds: 5
    stratify: true
    
  temporal_split:
    method: "time_series"
    test_size: 0.2
    
# Metrics for evaluation
metrics:
  primary:
    - name: "carbon_efficiency"
      type: "continuous"
      range: [0, 1]
      higher_is_better: true
      
    - name: "energy_consumption"
      type: "continuous"
      range: [0, 200]
      higher_is_better: false
      unit: "watts"
      
    - name: "performance_score"
      type: "continuous"
      range: [0, 1]
      higher_is_better: true
      
  secondary:
    - name: "resource_utilization"
      type: "continuous"
      range: [0, 100]
      higher_is_better: true
      unit: "percentage"
      
    - name: "response_time"
      type: "continuous"
      range: [0, 1000]
      higher_is_better: false
      unit: "milliseconds"
      
    - name: "throughput"
      type: "continuous"
      range: [0, 2000]
      higher_is_better: true
      unit: "requests_per_second"

# Experiment configurations
experiments:
  baseline_comparison:
    datasets:
      - "baseline_kubernetes_default"
      - "baseline_carbon_aware_v1"
    metrics:
      - "carbon_efficiency"
      - "energy_consumption"
      - "performance_score"
    statistical_tests:
      - "t_test"
      - "mann_whitney"
      - "effect_size"
      
  ablation_study:
    datasets:
      - "ablation_without_carbon_awareness"
      - "ablation_without_energy_optimization"
      - "ablation_full_config"
    metrics:
      - "carbon_efficiency"
      - "performance_score"
    analysis_type: "feature_importance"
    
  temporal_analysis:
    datasets:
      - "timeseries_14days"
    metrics:
      - "carbon_efficiency"
      - "energy_consumption"
    analysis_type: "trend_detection"
    seasonality: true

# Storage and caching
storage:
  cache_processed: true
  cache_directory: "processed"
  compression: true
  format: "parquet"  # for processed data
  
# Documentation
documentation:
  auto_generate: true
  include_statistics: true
  include_visualizations: true
  output_format: "html"