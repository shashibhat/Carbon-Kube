{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Comparison Analysis\n",
    "\n",
    "This notebook provides comprehensive baseline comparison analysis for carbon-efficient Kubernetes scheduling algorithms. We'll compare different scheduling approaches against established baselines to understand relative performance and improvements.\n",
    "\n",
    "## Baseline Categories\n",
    "\n",
    "We'll analyze several baseline categories:\n",
    "\n",
    "1. **Default Kubernetes Scheduler**: Standard Kubernetes scheduling\n",
    "2. **Carbon-Aware Schedulers**: Schedulers that consider carbon footprint\n",
    "3. **Energy-Efficient Schedulers**: Schedulers optimized for energy consumption\n",
    "4. **Performance-Optimized Schedulers**: Schedulers focused on performance metrics\n",
    "5. **Hybrid Approaches**: Schedulers balancing multiple objectives\n",
    "\n",
    "## Analysis Objectives\n",
    "\n",
    "- **Performance Comparison**: How do different approaches compare?\n",
    "- **Trade-off Analysis**: What are the trade-offs between different objectives?\n",
    "- **Statistical Validation**: Are the differences statistically significant?\n",
    "- **Scenario Analysis**: How do baselines perform under different conditions?\n",
    "- **Recommendation Generation**: Which baseline is best for specific use cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import json\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"Set2\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Baseline Datasets\n",
    "\n",
    "Let's load all available baseline datasets for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline datasets\n",
    "data_path = Path('../data')\n",
    "baseline_datasets = {}\n",
    "\n",
    "# Define baseline scenarios\n",
    "baseline_scenarios = [\n",
    "    'kubernetes_default',\n",
    "    'carbon_aware_v1',\n",
    "    'carbon_aware_v2',\n",
    "    'energy_efficient',\n",
    "    'performance_optimized',\n",
    "    'hybrid_balanced',\n",
    "    'cost_optimized'\n",
    "]\n",
    "\n",
    "for scenario in baseline_scenarios:\n",
    "    try:\n",
    "        path = data_path / 'synthetic' / f'baseline_{scenario}.csv'\n",
    "        baseline_datasets[scenario] = pd.read_csv(path)\n",
    "        print(f\"‚úÖ Loaded {scenario}: {len(baseline_datasets[scenario])} samples\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Could not load {scenario}\")\n",
    "\n",
    "print(f\"\\nüìä Loaded {len(baseline_datasets)} baseline datasets\")\n",
    "\n",
    "# Also load benchmark datasets\n",
    "benchmark_datasets = {}\n",
    "benchmark_scenarios = ['industry_standard', 'research_baseline', 'best_practice']\n",
    "\n",
    "for scenario in benchmark_scenarios:\n",
    "    try:\n",
    "        path = data_path / 'benchmarks' / f'{scenario}.csv'\n",
    "        benchmark_datasets[scenario] = pd.read_csv(path)\n",
    "        print(f\"‚úÖ Loaded benchmark {scenario}: {len(benchmark_datasets[scenario])} samples\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Could not load benchmark {scenario}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all datasets for comprehensive analysis\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "# Add baseline datasets\n",
    "for scenario, df in baseline_datasets.items():\n",
    "    df_copy = df.copy()\n",
    "    df_copy['baseline_type'] = scenario\n",
    "    df_copy['dataset_category'] = 'baseline'\n",
    "    combined_data = pd.concat([combined_data, df_copy], ignore_index=True)\n",
    "\n",
    "# Add benchmark datasets\n",
    "for scenario, df in benchmark_datasets.items():\n",
    "    df_copy = df.copy()\n",
    "    df_copy['baseline_type'] = scenario\n",
    "    df_copy['dataset_category'] = 'benchmark'\n",
    "    combined_data = pd.concat([combined_data, df_copy], ignore_index=True)\n",
    "\n",
    "if not combined_data.empty:\n",
    "    print(f\"üìà Combined dataset: {len(combined_data)} samples\")\n",
    "    print(f\"üè∑Ô∏è Baseline types: {combined_data['baseline_type'].unique()}\")\n",
    "    print(f\"üìÇ Dataset categories: {combined_data['dataset_category'].unique()}\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(\"\\nüìã Sample Data:\")\n",
    "    display(combined_data.head())\nelse:\n",
    "    print(\"‚ùå No data loaded for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Performance Overview\n",
    "\n",
    "Let's get an overview of how different baselines perform across key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance summary by baseline\n",
    "if not combined_data.empty:\n",
    "    performance_summary = combined_data.groupby('baseline_type')[[\n",
    "        'carbon_efficiency', 'energy_consumption', 'performance_score',\n",
    "        'response_time', 'throughput', 'resource_utilization'\n",
    "    ]].agg(['mean', 'std', 'min', 'max', 'count']).round(3)\n",
    "    \n",
    "    print(\"üìä Baseline Performance Summary:\")\n",
    "    display(performance_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison visualization\n",
    "if not combined_data.empty:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    metrics = ['carbon_efficiency', 'energy_consumption', 'performance_score',\n",
    "               'response_time', 'throughput', 'resource_utilization']\n",
    "    titles = ['Carbon Efficiency', 'Energy Consumption (W)', 'Performance Score',\n",
    "              'Response Time (ms)', 'Throughput (req/s)', 'Resource Utilization (%)']\n",
    "    \n",
    "    for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        row, col = i // 3, i % 3\n",
    "        \n",
    "        # Box plot by baseline type\n",
    "        combined_data.boxplot(column=metric, by='baseline_type', ax=axes[row, col])\n",
    "        axes[row, col].set_title(f'{title} by Baseline')\n",
    "        axes[row, col].set_xlabel('Baseline Type')\n",
    "        axes[row, col].set_ylabel(title)\n",
    "        axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.suptitle('')  # Remove automatic title\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detailed Baseline Comparison\n",
    "\n",
    "Let's perform detailed comparisons between specific baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline comparison matrix\n",
    "if not combined_data.empty:\n",
    "    # Calculate mean values for each baseline\n",
    "    baseline_means = combined_data.groupby('baseline_type')[[\n",
    "        'carbon_efficiency', 'energy_consumption', 'performance_score'\n",
    "    ]].mean()\n",
    "    \n",
    "    print(\"üèÜ Baseline Comparison Matrix (Mean Values):\")\n",
    "    display(baseline_means.round(3))\n",
    "    \n",
    "    # Rank baselines by each metric\n",
    "    rankings = pd.DataFrame()\n",
    "    rankings['Carbon Efficiency Rank'] = baseline_means['carbon_efficiency'].rank(ascending=False)\n",
    "    rankings['Energy Efficiency Rank'] = baseline_means['energy_consumption'].rank(ascending=True)  # Lower is better\n",
    "    rankings['Performance Rank'] = baseline_means['performance_score'].rank(ascending=False)\n",
    "    rankings['Overall Rank'] = (rankings['Carbon Efficiency Rank'] + \n",
    "                               rankings['Energy Efficiency Rank'] + \n",
    "                               rankings['Performance Rank']) / 3\n",
    "    \n",
    "    rankings = rankings.sort_values('Overall Rank')\n",
    "    \n",
    "    print(\"\\nü•á Baseline Rankings (1 = Best):\")\n",
    "    display(rankings.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize baseline comparison\n",
    "if not combined_data.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Carbon efficiency comparison\n",
    "    baseline_means['carbon_efficiency'].plot(kind='bar', ax=axes[0, 0], color='green', alpha=0.7)\n",
    "    axes[0, 0].set_title('Carbon Efficiency by Baseline')\n",
    "    axes[0, 0].set_ylabel('Carbon Efficiency')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Energy consumption comparison\n",
    "    baseline_means['energy_consumption'].plot(kind='bar', ax=axes[0, 1], color='red', alpha=0.7)\n",
    "    axes[0, 1].set_title('Energy Consumption by Baseline')\n",
    "    axes[0, 1].set_ylabel('Energy Consumption (W)')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Performance score comparison\n",
    "    baseline_means['performance_score'].plot(kind='bar', ax=axes[1, 0], color='blue', alpha=0.7)\n",
    "    axes[1, 0].set_title('Performance Score by Baseline')\n",
    "    axes[1, 0].set_ylabel('Performance Score')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Overall ranking\n",
    "    rankings['Overall Rank'].plot(kind='bar', ax=axes[1, 1], color='orange', alpha=0.7)\n",
    "    axes[1, 1].set_title('Overall Ranking (Lower = Better)')\n",
    "    axes[1, 1].set_ylabel('Average Rank')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Significance Testing\n",
    "\n",
    "Let's test if the differences between baselines are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pairwise_tests(df, metric, baseline_col='baseline_type', reference_baseline='kubernetes_default'):\n",
    "    \"\"\"Perform pairwise statistical tests against a reference baseline\"\"\"\n",
    "    if reference_baseline not in df[baseline_col].values:\n",
    "        print(f\"‚ùå Reference baseline '{reference_baseline}' not found\")\n",
    "        return None\n",
    "    \n",
    "    reference_data = df[df[baseline_col] == reference_baseline][metric]\n",
    "    results = []\n",
    "    \n",
    "    for baseline in df[baseline_col].unique():\n",
    "        if baseline == reference_baseline:\n",
    "            continue\n",
    "            \n",
    "        baseline_data = df[df[baseline_col] == baseline][metric]\n",
    "        \n",
    "        # T-test\n",
    "        t_stat, t_pval = stats.ttest_ind(reference_data, baseline_data)\n",
    "        \n",
    "        # Mann-Whitney U test\n",
    "        u_stat, u_pval = stats.mannwhitneyu(reference_data, baseline_data, alternative='two-sided')\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        pooled_std = np.sqrt(((len(reference_data) - 1) * reference_data.var() + \n",
    "                             (len(baseline_data) - 1) * baseline_data.var()) / \n",
    "                            (len(reference_data) + len(baseline_data) - 2))\n",
    "        cohens_d = (baseline_data.mean() - reference_data.mean()) / pooled_std\n",
    "        \n",
    "        # Improvement percentage\n",
    "        improvement = ((baseline_data.mean() - reference_data.mean()) / reference_data.mean()) * 100\n",
    "        \n",
    "        results.append({\n",
    "            'Baseline': baseline,\n",
    "            'Reference Mean': reference_data.mean(),\n",
    "            'Baseline Mean': baseline_data.mean(),\n",
    "            'Improvement (%)': improvement,\n",
    "            'T-test p-value': t_pval,\n",
    "            'Mann-Whitney p-value': u_pval,\n",
    "            'Effect Size (Cohen\\'s d)': cohens_d,\n",
    "            'Significant (p<0.05)': t_pval < 0.05,\n",
    "            'Effect Size Category': 'Small' if abs(cohens_d) < 0.5 else 'Medium' if abs(cohens_d) < 0.8 else 'Large'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "if not combined_data.empty:\n",
    "    # Test carbon efficiency improvements\n",
    "    carbon_tests = perform_pairwise_tests(combined_data, 'carbon_efficiency')\n",
    "    \n",
    "    if carbon_tests is not None:\n",
    "        print(\"üß™ Statistical Tests - Carbon Efficiency vs Kubernetes Default:\")\n",
    "        display(carbon_tests.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test energy consumption improvements\n",
    "if not combined_data.empty:\n",
    "    energy_tests = perform_pairwise_tests(combined_data, 'energy_consumption')\n",
    "    \n",
    "    if energy_tests is not None:\n",
    "        print(\"üß™ Statistical Tests - Energy Consumption vs Kubernetes Default:\")\n",
    "        display(energy_tests.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance score improvements\n",
    "if not combined_data.empty:\n",
    "    performance_tests = perform_pairwise_tests(combined_data, 'performance_score')\n",
    "    \n",
    "    if performance_tests is not None:\n",
    "        print(\"üß™ Statistical Tests - Performance Score vs Kubernetes Default:\")\n",
    "        display(performance_tests.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trade-off Analysis\n",
    "\n",
    "Let's analyze the trade-offs between different objectives (carbon efficiency, energy consumption, performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trade-off analysis\n",
    "if not combined_data.empty:\n",
    "    # Calculate correlation between metrics\n",
    "    correlation_matrix = combined_data[[\n",
    "        'carbon_efficiency', 'energy_consumption', 'performance_score',\n",
    "        'response_time', 'throughput', 'resource_utilization'\n",
    "    ]].corr()\n",
    "    \n",
    "    print(\"üîÑ Metric Correlation Analysis:\")\n",
    "    display(correlation_matrix.round(3))\n",
    "    \n",
    "    # Visualize correlations\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='RdBu_r', center=0, \n",
    "                square=True, linewidths=0.5)\n",
    "    plt.title('Metric Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for trade-off analysis\n",
    "if not combined_data.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Carbon efficiency vs Energy consumption\n",
    "    for baseline in combined_data['baseline_type'].unique():\n",
    "        data = combined_data[combined_data['baseline_type'] == baseline]\n",
    "        axes[0, 0].scatter(data['carbon_efficiency'], data['energy_consumption'], \n",
    "                          label=baseline, alpha=0.6, s=50)\n",
    "    axes[0, 0].set_xlabel('Carbon Efficiency')\n",
    "    axes[0, 0].set_ylabel('Energy Consumption (W)')\n",
    "    axes[0, 0].set_title('Carbon Efficiency vs Energy Consumption')\n",
    "    axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Carbon efficiency vs Performance\n",
    "    for baseline in combined_data['baseline_type'].unique():\n",
    "        data = combined_data[combined_data['baseline_type'] == baseline]\n",
    "        axes[0, 1].scatter(data['carbon_efficiency'], data['performance_score'], \n",
    "                          label=baseline, alpha=0.6, s=50)\n",
    "    axes[0, 1].set_xlabel('Carbon Efficiency')\n",
    "    axes[0, 1].set_ylabel('Performance Score')\n",
    "    axes[0, 1].set_title('Carbon Efficiency vs Performance')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Energy consumption vs Performance\n",
    "    for baseline in combined_data['baseline_type'].unique():\n",
    "        data = combined_data[combined_data['baseline_type'] == baseline]\n",
    "        axes[1, 0].scatter(data['energy_consumption'], data['performance_score'], \n",
    "                          label=baseline, alpha=0.6, s=50)\n",
    "    axes[1, 0].set_xlabel('Energy Consumption (W)')\n",
    "    axes[1, 0].set_ylabel('Performance Score')\n",
    "    axes[1, 0].set_title('Energy Consumption vs Performance')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3D-like view: Response time vs Throughput colored by Carbon efficiency\n",
    "    scatter = axes[1, 1].scatter(combined_data['response_time'], combined_data['throughput'], \n",
    "                                c=combined_data['carbon_efficiency'], cmap='viridis', \n",
    "                                alpha=0.6, s=50)\n",
    "    axes[1, 1].set_xlabel('Response Time (ms)')\n",
    "    axes[1, 1].set_ylabel('Throughput (req/s)')\n",
    "    axes[1, 1].set_title('Response Time vs Throughput (colored by Carbon Efficiency)')\n",
    "    plt.colorbar(scatter, ax=axes[1, 1], label='Carbon Efficiency')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scenario-Based Analysis\n",
    "\n",
    "Let's analyze how different baselines perform under various scenarios (workload types, node types, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by workload type\n",
    "if not combined_data.empty and 'workload_type' in combined_data.columns:\n",
    "    workload_analysis = combined_data.groupby(['baseline_type', 'workload_type'])[[\n",
    "        'carbon_efficiency', 'energy_consumption', 'performance_score'\n",
    "    ]].mean().round(3)\n",
    "    \n",
    "    print(\"üìã Performance by Workload Type:\")\n",
    "    display(workload_analysis)\n",
    "    \n",
    "    # Visualize workload-specific performance\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    metrics = ['carbon_efficiency', 'energy_consumption', 'performance_score']\n",
    "    titles = ['Carbon Efficiency', 'Energy Consumption', 'Performance Score']\n",
    "    \n",
    "    for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        # Create pivot table for heatmap\n",
    "        pivot_data = combined_data.pivot_table(\n",
    "            values=metric, \n",
    "            index='baseline_type', \n",
    "            columns='workload_type', \n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='RdYlBu_r', ax=axes[i])\n",
    "        axes[i].set_title(f'{title} by Baseline and Workload')\n",
    "        axes[i].set_xlabel('Workload Type')\n",
    "        axes[i].set_ylabel('Baseline Type')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by node type\n",
    "if not combined_data.empty and 'node_type' in combined_data.columns:\n",
    "    node_analysis = combined_data.groupby(['baseline_type', 'node_type'])[[\n",
    "        'carbon_efficiency', 'energy_consumption', 'performance_score'\n",
    "    ]].mean().round(3)\n",
    "    \n",
    "    print(\"üñ•Ô∏è Performance by Node Type:\")\n",
    "    display(node_analysis)\n",
    "    \n",
    "    # Find best baseline for each node type\n",
    "    best_by_node = {}\n",
    "    for node_type in combined_data['node_type'].unique():\n",
    "        node_data = combined_data[combined_data['node_type'] == node_type]\n",
    "        best_carbon = node_data.groupby('baseline_type')['carbon_efficiency'].mean().idxmax()\n",
    "        best_energy = node_data.groupby('baseline_type')['energy_consumption'].mean().idxmin()\n",
    "        best_perf = node_data.groupby('baseline_type')['performance_score'].mean().idxmax()\n",
    "        \n",
    "        best_by_node[node_type] = {\n",
    "            'best_carbon': best_carbon,\n",
    "            'best_energy': best_energy,\n",
    "            'best_performance': best_perf\n",
    "        }\n",
    "    \n",
    "    print(\"\\nüèÜ Best Baseline by Node Type:\")\n",
    "    for node_type, bests in best_by_node.items():\n",
    "        print(f\"  {node_type}:\")\n",
    "        print(f\"    Carbon: {bests['best_carbon']}\")\n",
    "        print(f\"    Energy: {bests['best_energy']}\")\n",
    "        print(f\"    Performance: {bests['best_performance']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Principal Component Analysis\n",
    "\n",
    "Let's use PCA to understand the main dimensions of variation between baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA analysis\n",
    "if not combined_data.empty:\n",
    "    # Select numeric features for PCA\n",
    "    numeric_features = [\n",
    "        'carbon_efficiency', 'energy_consumption', 'performance_score',\n",
    "        'cpu_utilization', 'memory_utilization', 'response_time', 'throughput'\n",
    "    ]\n",
    "    \n",
    "    # Filter available features\n",
    "    available_features = [f for f in numeric_features if f in combined_data.columns]\n",
    "    \n",
    "    if len(available_features) >= 3:\n",
    "        # Prepare data\n",
    "        pca_data = combined_data[available_features].dropna()\n",
    "        baseline_labels = combined_data.loc[pca_data.index, 'baseline_type']\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        pca_data_scaled = scaler.fit_transform(pca_data)\n",
    "        \n",
    "        # Perform PCA\n",
    "        pca = PCA(n_components=min(len(available_features), 4))\n",
    "        pca_result = pca.fit_transform(pca_data_scaled)\n",
    "        \n",
    "        # Print explained variance\n",
    "        print(\"üìä PCA Analysis Results:\")\n",
    "        print(f\"Explained variance ratio: {pca.explained_variance_ratio_.round(3)}\")\n",
    "        print(f\"Cumulative explained variance: {pca.explained_variance_ratio_.cumsum().round(3)}\")\n",
    "        \n",
    "        # Create PCA visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # PCA scatter plot\n",
    "        unique_baselines = baseline_labels.unique()\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_baselines)))\n",
    "        \n",
    "        for i, baseline in enumerate(unique_baselines):\n",
    "            mask = baseline_labels == baseline\n",
    "            axes[0].scatter(pca_result[mask, 0], pca_result[mask, 1], \n",
    "                           c=[colors[i]], label=baseline, alpha=0.7, s=50)\n",
    "        \n",
    "        axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "        axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "        axes[0].set_title('PCA: Baseline Clustering')\n",
    "        axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Feature importance in PCA\n",
    "        feature_importance = pd.DataFrame(\n",
    "            pca.components_[:2].T,\n",
    "            columns=['PC1', 'PC2'],\n",
    "            index=available_features\n",
    "        )\n",
    "        \n",
    "        feature_importance.plot(kind='bar', ax=axes[1])\n",
    "        axes[1].set_title('Feature Contributions to Principal Components')\n",
    "        axes[1].set_ylabel('Component Loading')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nüéØ Feature Contributions to PC1 and PC2:\")\n",
    "        display(feature_importance.round(3))\n",
    "    else:\n",
    "        print(\"‚ùå Insufficient numeric features for PCA analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Recommendations and Best Practices\n",
    "\n",
    "Based on our comprehensive baseline analysis, let's generate recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_baseline_recommendations(combined_data, rankings, statistical_tests):\n",
    "    \"\"\"Generate comprehensive recommendations based on baseline analysis\"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    if not combined_data.empty and rankings is not None:\n",
    "        # Overall best performer\n",
    "        best_overall = rankings.index[0]\n",
    "        recommendations.append(\n",
    "            f\"üèÜ Overall Best Performer: '{best_overall}' - Best balanced performance across all metrics\"\n",
    "        )\n",
    "        \n",
    "        # Best for specific objectives\n",
    "        best_carbon = rankings['Carbon Efficiency Rank'].idxmin()\n",
    "        best_energy = rankings['Energy Efficiency Rank'].idxmin()\n",
    "        best_performance = rankings['Performance Rank'].idxmin()\n",
    "        \n",
    "        recommendations.extend([\n",
    "            f\"üå± Best for Carbon Efficiency: '{best_carbon}' - Prioritize for environmental goals\",\n",
    "            f\"‚ö° Best for Energy Efficiency: '{best_energy}' - Prioritize for cost reduction\",\n",
    "            f\"üöÄ Best for Performance: '{best_performance}' - Prioritize for application responsiveness\"\n",
    "        ])\n",
    "    \n",
    "    # Statistical significance insights\n",
    "    if statistical_tests is not None and not statistical_tests.empty:\n",
    "        significant_improvements = statistical_tests[\n",
    "            (statistical_tests['Significant (p<0.05)'] == True) & \n",
    "            (statistical_tests['Improvement (%)'] > 0)\n",
    "        ]\n",
    "        \n",
    "        if not significant_improvements.empty:\n",
    "            best_improvement = significant_improvements.loc[significant_improvements['Improvement (%)'].idxmax()]\n",
    "            recommendations.append(\n",
    "                f\"üìà Largest Significant Improvement: '{best_improvement['Baseline']}' \"\n",
    "                f\"({best_improvement['Improvement (%)']:.1f}% improvement)\"\n",
    "            )\n",
    "    \n",
    "    # Workload-specific recommendations\n",
    "    if 'workload_type' in combined_data.columns:\n",
    "        for workload in combined_data['workload_type'].unique():\n",
    "            workload_data = combined_data[combined_data['workload_type'] == workload]\n",
    "            best_for_workload = workload_data.groupby('baseline_type')['carbon_efficiency'].mean().idxmax()\n",
    "            recommendations.append(\n",
    "                f\"üìã Best for '{workload}' workloads: '{best_for_workload}'\"\n",
    "            )\n",
    "    \n",
    "    # General best practices\n",
    "    recommendations.extend([\n",
    "        \"üéØ Consider your primary objective when selecting a baseline\",\n",
    "        \"‚öñÔ∏è Evaluate trade-offs between carbon efficiency, energy consumption, and performance\",\n",
    "        \"üîÑ Test baselines under your specific workload conditions\",\n",
    "        \"üìä Monitor baseline performance continuously in production\",\n",
    "        \"üß™ Conduct A/B tests when switching between baselines\",\n",
    "        \"üìà Consider hybrid approaches for complex multi-objective scenarios\",\n",
    "        \"üîç Validate statistical significance before making production changes\"\n",
    "    ])\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = generate_baseline_recommendations(\n",
    "    combined_data,\n",
    "    rankings if 'rankings' in locals() else None,\n",
    "    carbon_tests if 'carbon_tests' in locals() else None\n",
    ")\n",
    "\n",
    "print(\"üí° Baseline Comparison Recommendations:\")\n",
    "print(\"=\" * 60)\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Baseline Comparison Results\n",
    "\n",
    "Let's save our comprehensive baseline analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results summary\n",
    "baseline_results = {\n",
    "    'analysis_info': {\n",
    "        'analysis_date': datetime.now().isoformat(),\n",
    "        'total_samples': len(combined_data) if not combined_data.empty else 0,\n",
    "        'baselines_tested': combined_data['baseline_type'].unique().tolist() if not combined_data.empty else [],\n",
    "        'reference_baseline': 'kubernetes_default'\n",
    "    },\n",
    "    'performance_summary': performance_summary.to_dict() if 'performance_summary' in locals() else {},\n",
    "    'baseline_rankings': rankings.to_dict() if 'rankings' in locals() else {},\n",
    "    'statistical_tests': {\n",
    "        'carbon_efficiency': carbon_tests.to_dict('records') if 'carbon_tests' in locals() and carbon_tests is not None else [],\n",
    "        'energy_consumption': energy_tests.to_dict('records') if 'energy_tests' in locals() and energy_tests is not None else [],\n",
    "        'performance_score': performance_tests.to_dict('records') if 'performance_tests' in locals() and performance_tests is not None else []\n",
    "    },\n",
    "    'correlation_analysis': correlation_matrix.to_dict() if 'correlation_matrix' in locals() else {},\n",
    "    'workload_analysis': workload_analysis.to_dict() if 'workload_analysis' in locals() else {},\n",
    "    'node_analysis': node_analysis.to_dict() if 'node_analysis' in locals() else {},\n",
    "    'pca_analysis': {\n",
    "        'explained_variance_ratio': pca.explained_variance_ratio_.tolist() if 'pca' in locals() else [],\n",
    "        'feature_contributions': feature_importance.to_dict() if 'feature_importance' in locals() else {}\n",
    "    },\n",
    "    'recommendations': recommendations\n",
    "}\n",
    "\n",
    "# Save results\n",
    "results_path = Path('../results')\n",
    "results_path.mkdir(exist_ok=True)\n",
    "\n",
    "with open(results_path / 'baseline_comparison_results.json', 'w') as f:\n",
    "    json.dump(baseline_results, f, indent=2, default=str)\n",
    "\n",
    "print(\"üíæ Baseline comparison results saved to: evaluation/results/baseline_comparison_results.json\")\n",
    "\n",
    "# Save rankings as CSV\n",
    "if 'rankings' in locals():\n",
    "    rankings.to_csv(results_path / 'baseline_rankings.csv')\n",
    "    print(\"üèÜ Baseline rankings saved to: evaluation/results/baseline_rankings.csv\")\n",
    "\n",
    "# Save statistical test results\n",
    "if 'carbon_tests' in locals() and carbon_tests is not None:\n",
    "    carbon_tests.to_csv(results_path / 'carbon_efficiency_tests.csv', index=False)\n",
    "    print(\"üß™ Statistical test results saved to: evaluation/results/carbon_efficiency_tests.csv\")\n",
    "\n",
    "print(\"\\n‚úÖ Baseline comparison analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This comprehensive baseline comparison analysis has provided insights into:\n",
    "\n",
    "### Key Findings:\n",
    "1. **Performance Rankings**: Identified the best-performing baselines across different metrics\n",
    "2. **Statistical Significance**: Determined which improvements are statistically reliable\n",
    "3. **Trade-off Analysis**: Understood the relationships between different performance objectives\n",
    "4. **Scenario-Specific Performance**: Found optimal baselines for different workload and node types\n",
    "5. **Principal Components**: Identified the main dimensions of variation between baselines\n",
    "\n",
    "### Decision Framework:\n",
    "- **Carbon-First**: Choose baselines optimized for carbon efficiency\n",
    "- **Cost-First**: Choose baselines optimized for energy consumption\n",
    "- **Performance-First**: Choose baselines optimized for application performance\n",
    "- **Balanced**: Choose baselines with the best overall ranking\n",
    "\n",
    "### Next Steps:\n",
    "1. **Production Testing**: Validate findings in your specific environment\n",
    "2. **Monitoring**: Implement continuous monitoring of baseline performance\n",
    "3. **Optimization**: Fine-tune selected baselines for your use case\n",
    "4. **Regular Review**: Periodically re-evaluate baseline performance\n",
    "\n",
    "### Related Notebooks:\n",
    "- **01_Getting_Started.ipynb**: Basic framework introduction\n",
    "- **02_Ablation_Studies.ipynb**: Feature importance analysis\n",
    "- **04_Statistical_Analysis.ipynb**: Advanced statistical methods\n",
    "\n",
    "Use these insights to make informed decisions about which baseline scheduler to deploy in your Kubernetes environment! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}